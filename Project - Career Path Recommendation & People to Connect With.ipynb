{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dae446c-2b80-49fd-bf91-20696025b666",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Career Path Recommendations & People to Connect With"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82b97b7b-d17e-4f7e-9086-8e34ac343913",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Preprocess\n",
    "\n",
    "Assumptions:\n",
    "1. If a LinkedIn user hasn't provided their start date in the format month + year and has used only the year, we assume they started working on 01.01.YEAR. In all other cases, we assume they started working on 01.MONTH.YEAR.\n",
    "2. The date used to order positions is the start date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b9d1489-d9db-449e-9fdb-a27eac450265",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import sparknlp\n",
    "from pyspark.sql.functions import col, expr, explode, when, split, length,  to_date, concat_ws, lower\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from collections import Counter\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51fc9118-d8bd-4313-a3ab-dc09d64ade68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "spark = sparknlp.start()\n",
    "profiles = spark.read.parquet('/dbfs/linkedin_people_train_data')\n",
    "companies = spark.read.parquet('/dbfs/linkedin_train_data')\n",
    "profiels_followers = profiles.select(\"id\", \"followers\")\n",
    "levels_to_remove = [\"senior\", \"junior\", \"sr\", \"i\", \"ii\"]\n",
    "regex_pattern = \"|\".join(levels_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8a4d19c-2600-4bc3-b299-5664b90f560e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2768313\n1339790\n"
     ]
    }
   ],
   "source": [
    "print(profiles.count())\n",
    "print(companies.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "194ae60d-21b9-48a7-9473-ea52d1cdc0ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "companies.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "907ea930-88f5-46c6-9636-5fcb8b6e603e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols_to_drop = ['about', 'avatar', 'certifications', 'city', 'country_code', 'current_company', 'current_company:company_id', 'current_company:industry', 'current_company:name', 'education', 'educations_details', 'followers', 'following', 'groups', 'image','languages', 'people_also_viewed', 'posts', 'recommendations', 'recommendations_count', 'timestamp', 'url', 'volunteer_experience', 'courses']\n",
    "\n",
    "profiles = profiles.drop(*cols_to_drop)\n",
    "positions = profiles.select(col(\"id\"), col(\"name\"), explode(\"experience.positions\").alias(\"position\"))\n",
    "positions = positions.withColumn(\"position\", expr(\"filter(position, x -> x.title is not null)\"))\n",
    "\n",
    "# Keep only start_date and title in position\n",
    "positions = positions.withColumn(\"position\", expr(\"transform(position, x -> struct(x.start_date, x.title))\"))\n",
    "\n",
    "# Explode position array to separate rows\n",
    "positions = positions.select(col(\"id\"), col(\"name\"), explode(\"position\").alias(\"position\"))\n",
    "\n",
    "# Add columns for start_date and title\n",
    "positions = positions.withColumn(\"start_date\", col(\"position.start_date\")).withColumn(\"title\", col(\"position.title\"))\n",
    "\n",
    "# Drop row if the value in position is null\n",
    "positions = positions.filter(positions.position.isNotNull())\n",
    "positions.limit(100).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "429a6641-c158-4476-907c-e5b467f3badd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "month_map = {\n",
    "    \"Jan\": \"1\", \"Feb\": \"2\", \"Mar\": \"3\", \"Apr\": \"4\", \"May\": \"5\", \"Jun\": \"6\",\n",
    "    \"Jul\": \"7\", \"Aug\": \"8\", \"Sep\": \"9\", \"Oct\": \"10\", \"Nov\": \"11\", \"Dec\": \"12\"\n",
    "}\n",
    "\n",
    "positions_df = positions.withColumn(\n",
    "    \"start_month\", \n",
    "    when(length(col(\"start_date\")) == 4, \"1\").otherwise(split(col(\"start_date\"), \" \")[0])\n",
    ")\n",
    "\n",
    "for month, num in month_map.items():\n",
    "    positions_df = positions_df.withColumn(\n",
    "        \"start_month\", \n",
    "        when(col(\"start_month\") == month, num).otherwise(col(\"start_month\"))\n",
    "    )\n",
    "\n",
    "positions_df = positions_df.withColumn(\n",
    "    \"start_year\", \n",
    "    when(length(col(\"start_date\")) == 4, col(\"start_date\")).otherwise(split(col(\"start_date\"), \" \")[1])\n",
    ")\n",
    "\n",
    "positions_df = positions_df.withColumn(\"start date\", to_date(concat_ws(\"-\", col(\"start_year\"), col(\"start_month\"), expr(\"1\")), \"yyyy-M-d\"))\n",
    "drop_cols = [\"start_date\", \"start_month\", \"start_year\"]\n",
    "positions_df = positions_df.drop(*drop_cols)\n",
    "positions_df.limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c56221d4-288d-4c3a-897d-e05723257772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = (positions_df\n",
    "    .groupBy(\"name\", \"id\")\n",
    "    .agg(\n",
    "        F.sort_array(\n",
    "            F.collect_list(\n",
    "                F.struct(\n",
    "                    F.col(\"start date\").alias(\"start_date\"),\n",
    "                    F.col(\"position.title\").alias(\"title\")\n",
    "                )\n",
    "            ),\n",
    "            asc=True\n",
    "        ).alias(\"position_history\")\n",
    "    )\n",
    "    .select(\n",
    "        \"name\", \n",
    "        \"id\",\n",
    "        F.col(\"position_history.title\").alias(\"titles\"),\n",
    "        F.col(\"position_history.start_date\").alias(\"start_dates\")\n",
    "    )\n",
    ")\n",
    "\n",
    "display(result_df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b871b51b-3198-4278-9de9-5bac08ceb0b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Select your dream job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3133780e-b0a0-4fba-9c15-0456b860ef97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#USER SELECTIONS\n",
    "#dream_job = 'data scientist'\n",
    "#dream_job = 'data engineer'\n",
    "#dream_job = 'hrbp'\n",
    "dream_job = 'data analyst'\n",
    "num_of_steps = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bb5a4a0-6ed3-48a9-a6c8-689544b80e75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "relevant_people = positions_df.filter(lower(col('title')).contains(dream_job)).select('name')\n",
    "relevant_people = relevant_people.withColumnRenamed(\"name\", \"name_relevant\")\n",
    "#extract all the rows of the relevant people from positions_df\n",
    "relevant_people_path = relevant_people.join(result_df, relevant_people.name_relevant == result_df.name, \"inner\")\n",
    "relevant_people_path = relevant_people_path.drop('name_relevant')\n",
    "display(relevant_people_path.limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1815845c-202e-40cf-ab8d-e7327a575119",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add a column for the path to the dream job\n",
    "relevant_people_path = relevant_people_path.withColumn(\n",
    "    \"path_to_dream_job\",\n",
    "    F.expr(\n",
    "        f\"\"\"\n",
    "        IF(\n",
    "            array_position(\n",
    "                transform(titles, x -> CASE WHEN lower(x) LIKE '%{dream_job.lower()}%' THEN 1 ELSE 0 END), \n",
    "                1\n",
    "            ) > 0,\n",
    "            slice(titles, 1, array_position(\n",
    "                transform(titles, x -> CASE WHEN lower(x) LIKE '%{dream_job.lower()}%' THEN 1 ELSE 0 END), \n",
    "                1\n",
    "            ) - 1),\n",
    "            array()\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add a column for the number of steps before the dream job\n",
    "relevant_people_path = relevant_people_path.withColumn(\n",
    "    \"steps_to_dream_job\",\n",
    "    F.expr(\n",
    "        f\"\"\"\n",
    "        IF(\n",
    "            array_position(\n",
    "                transform(titles, x -> CASE WHEN lower(x) LIKE '%{dream_job.lower()}%' THEN 1 ELSE 0 END), \n",
    "                1\n",
    "            ) > 0,\n",
    "            reverse(sequence(\n",
    "                1, \n",
    "                array_position(\n",
    "                    transform(titles, x -> CASE WHEN lower(x) LIKE '%{dream_job.lower()}%' THEN 1 ELSE 0 END), \n",
    "                    1\n",
    "                ) - 1\n",
    "            )),\n",
    "            array()\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Explode the array to create one row per step\n",
    "relevant_people_path = relevant_people_path.withColumn(\"step_title\", F.explode_outer(F.col(\"path_to_dream_job\")))\n",
    "\n",
    "# Add a column indicating the number of steps before the dream job for each title\n",
    "relevant_people_path = relevant_people_path.withColumn(\n",
    "    \"steps_before_dream_job\",\n",
    "    F.expr(f\"array_position(path_to_dream_job, step_title)\")\n",
    ")\n",
    "\n",
    "# Drop rows where path_to_dream_job is an empty list\n",
    "relevant_people_path = relevant_people_path.filter(F.size(F.col(\"path_to_dream_job\")) > 0)\n",
    "\n",
    "#drop duplicates\n",
    "relevant_people_path = relevant_people_path.dropDuplicates()\n",
    "# Display the resulting DataFrame\n",
    "\n",
    "#avg_steps_df = relevant_people_path.select(\"step_title\", \"steps_before_dream_job\")#.groupBy(\"step_title\").agg(F.avg(\"steps_before_dream_job\").alias(\"avg_steps_before_dream_job\")\n",
    "#)\n",
    "relevant_people_path.printSchema()\n",
    "# Do the same normalization as above to avg_steps_df\n",
    "relevant_people_path = relevant_people_path.withColumn(\n",
    "    \"step_title\",\n",
    "    F.lower(  # Convert to lowercase\n",
    "        F.trim(  # Remove leading and trailing spaces\n",
    "            F.regexp_replace(\n",
    "                F.regexp_replace(\n",
    "                    F.regexp_replace(\n",
    "                        F.regexp_replace(F.col(\"step_title\"), f\"(?i)[,/-]|\\\\b({regex_pattern})\\\\b\", \"\"),  # Clean special chars\n",
    "                        r\"\\.\",  # Replace '.' with an empty string\n",
    "                        \"\"\n",
    "                    ),\n",
    "                    r\"@\",  # Remove '@' characters\n",
    "                    \"\"\n",
    "                ),\n",
    "                r\"\\s+\",  # Replace multiple spaces with a single space\n",
    "                \" \"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "avg_steps_df = relevant_people_path.select(\"step_title\", \"steps_before_dream_job\")\n",
    "avg_steps_df = avg_steps_df.groupBy(\"step_title\").agg(F.avg(\"steps_before_dream_job\").alias(\"avg_steps_before_dream_job\"))\n",
    "\n",
    "avg_steps_df.limit(10).display()\n",
    "relevant_people_path.limit(30).display() #return 530 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a6521ad-3a9e-415f-a027-3d55acf6072c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Filter steps within 2 positions before the dream job\n",
    "up2_steps = relevant_people_path.filter(F.col(\"steps_before_dream_job\") <= num_of_steps)\n",
    "\n",
    "# Step 2: Select and collect the step titles\n",
    "up2_steps_positions_list = [row['step_title'] for row in up2_steps.collect()]\n",
    "\n",
    "# Step 3: Count occurrences of each position\n",
    "up2_steps_positions_list_counts = Counter(up2_steps_positions_list)\n",
    "\n",
    "# Step 4: Convert counts into a DataFrame for further processing\n",
    "up2_steps_positions_df = spark.createDataFrame(\n",
    "    [(position, count) for position, count in up2_steps_positions_list_counts.items()],\n",
    "    schema=[\"Position\", \"Count\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Step 5: Normalize the 'Position' column (cleaning and standardizing)\n",
    "normalized_up2_steps_positions_df = up2_steps_positions_df.select(\n",
    "    F.lower(  # Convert to lowercase\n",
    "        F.trim(  # Remove leading and trailing spaces\n",
    "            F.regexp_replace(\n",
    "                F.regexp_replace(\n",
    "                    F.regexp_replace(\n",
    "                        F.regexp_replace(F.col(\"Position\"), f\"(?i)[,/-]|\\\\b({regex_pattern})\\\\b\", \"\"),  # Clean up special chars\n",
    "                        r\"\\.\",  # Replace '.' with an empty string\n",
    "                        \"\"\n",
    "                    ),\n",
    "                    r\"@\",  # Remove '@' characters\n",
    "                    \"\"\n",
    "                ),\n",
    "                r\"\\s+\",  # Replace multiple spaces with a single space\n",
    "                \" \"\n",
    "            )\n",
    "        )\n",
    "    ).alias(\"Position\"),\n",
    "    F.col(\"Count\")\n",
    ")\n",
    "\n",
    "# Step 6: Group by the normalized 'Position' column and sum the 'Count'\n",
    "grouped_positions_df = normalized_up2_steps_positions_df.groupBy(\"Position\").agg(\n",
    "    F.sum(\"Count\").alias(\"Total Count\")\n",
    ")\n",
    "\n",
    "# Step 7: Sort the results for better visualization\n",
    "sorted_up2_steps_positions_df = grouped_positions_df.orderBy(\"Total Count\", ascending=False)\n",
    "\n",
    "sorted_up2_steps_positions_df = sorted_up2_steps_positions_df.join(avg_steps_df, sorted_up2_steps_positions_df.Position == avg_steps_df.step_title)\n",
    "\n",
    "sorted_up2_steps_positions_df = sorted_up2_steps_positions_df.select(\"Position\", \"Total Count\", \"avg_steps_before_dream_job\")\n",
    "\n",
    "# Step 8: Display the final sorted DataFrame\n",
    "sorted_up2_steps_positions_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d03c758-8bf8-422a-8b1f-d4d6ae3d0dbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Until here, we've extracted the positions that Linkedin users have done before they startes to work as the given dream position.\n",
    "Now, we'll orgenize the data so the user will be able to infer fro it what might help him. In the first step, we would like to group similar positions into one group. For this, we are using word2vec on the position name and DBSCAN to find clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97b638e2-c1e4-43ff-9c1f-ef3b58da1a1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove levels and unwanted marks from the \"Position\" column\n",
    "#positions_df = sorted_up2_steps_positions_df.select(\n",
    "#    F.trim(F.regexp_replace(F.col(\"Position\"), f\"(?i)[,/-]|\\\\b({regex_pattern})\\\\b\", \"\")).alias(\"Position\")\n",
    "#)\n",
    "#convert to lower case, and drop values that are not latters\n",
    "#positions_df = positions_df.withColumn(\"Position\", F.lower(F.col(\"Position\")))\n",
    "positions_df = sorted_up2_steps_positions_df.select(\"Position\")\n",
    "positions_list = [row[\"Position\"] for row in positions_df.collect()]\n",
    "#drop duplicates\n",
    "positions_list = list(set(positions_list))\n",
    "\n",
    "# Tokenize job titles (split into words)\n",
    "tokenized_positions = [[word.lower() for word in position.split()] for position in positions_list]\n",
    "\n",
    "# Step 2: Train Word2Vec\n",
    "word2vec = Word2Vec(vectorSize=70, minCount=1, inputCol=\"tokens\", outputCol=\"vector\")\n",
    "positions_spark_df = spark.createDataFrame(pd.DataFrame({\"tokens\": tokenized_positions, \"Position\": positions_list}))\n",
    "model = word2vec.fit(positions_spark_df)\n",
    "\n",
    "# Generate embeddings for each job title\n",
    "positions_with_vectors = model.transform(positions_spark_df)\n",
    "\n",
    "# Convert embeddings to a list for clustering\n",
    "embeddings = np.array(positions_with_vectors.select(\"vector\").rdd.map(lambda x: x[\"vector\"]).collect())\n",
    "\n",
    "# DBSCAN\n",
    "dbscan = DBSCAN(eps=0.08, min_samples=2, metric=\"cosine\") \n",
    "clusters = dbscan.fit_predict(embeddings)\n",
    "\n",
    "# Step 4: Add Cluster Labels\n",
    "positions_with_clusters = positions_with_vectors.withColumn(\"Cluster\", F.lit(-1))\n",
    "for idx, cluster in enumerate(clusters):\n",
    "    positions_with_clusters = positions_with_clusters.withColumn(\n",
    "        \"Cluster\",\n",
    "        F.when(F.col(\"Position\") == positions_list[idx], cluster).otherwise(F.col(\"Cluster\")),\n",
    "    )\n",
    "#positions_with_clusters.display()\n",
    "# Step 5: Group by Cluster\n",
    "positions_clusters = positions_with_clusters.groupBy(\"Cluster\").agg(F.collect_list(\"Position\").alias(\"Positions\"))\n",
    "\n",
    "#find a title, based on the 2 most common words (it the most commen word is more then twice common then the second commom words, used only the most common word)\n",
    "from collections import Counter\n",
    "\n",
    "# Define a set of stop words\n",
    "STOP_WORDS = {\"the\", \"is\", \"in\", \"and\", \"to\", \"a\", \"of\", \"on\", \"for\", \"with\", \"at\", \"by\", \"an\", \"be\", \"this\", \"that\"}\n",
    "\n",
    "def assign_cluster_title(cluster_id, positions):\n",
    "    if cluster_id == -1:\n",
    "        return \"Other\"  # Explicitly label outliers\n",
    "\n",
    "    all_words = [word.lower() for position in positions for word in position.split()]\n",
    "    filtered_words = [word for word in all_words if word not in STOP_WORDS]\n",
    "    word_counts = Counter(filtered_words)\n",
    "    most_common = word_counts.most_common(2)\n",
    "    \n",
    "    if len(most_common) == 0:\n",
    "        return \"Unknown\"  # No positions in cluster\n",
    "    \n",
    "    if len(most_common) == 1 or (most_common[0][1] > 2 * most_common[1][1] and most_common[1][1] > 1):\n",
    "        # Use only the most common word if it is more than twice as frequent\n",
    "        return most_common[0][0]\n",
    "    else:\n",
    "        # Otherwise, combine the two most common words\n",
    "        return f\"{most_common[0][0]} {most_common[1][0]}\"\n",
    "\n",
    "\n",
    "# Register the function as a UDF\n",
    "assign_cluster_title_udf = F.udf(assign_cluster_title)\n",
    "\n",
    "# Apply the UDF to assign titles\n",
    "positions_clusters_with_titles = positions_clusters.withColumn(\n",
    "    \"Cluster_Title\", \n",
    "    assign_cluster_title_udf(F.col(\"Cluster\"), F.col(\"Positions\"))\n",
    ")\n",
    "#add a col that counts the number of positions in Positiond col\n",
    "positions_clusters_with_titles = positions_clusters_with_titles.withColumn(\"Num of positions in cluster\", F.size(F.col(\"Positions\")))\n",
    "\n",
    "# Step 7: Display Final Clusters with Titles\n",
    "positions_clusters_with_titles.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6102dee0-2211-4f7b-a952-cd6071b63709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explode the Positions array in positions_clusters_with_titles\n",
    "exploded_df = positions_clusters_with_titles.select(\n",
    "    F.col(\"Cluster\"),\n",
    "    F.col(\"Cluster_Title\"),\n",
    "    F.col(\"Positions\"),\n",
    "    F.col(\"Num of positions in cluster\")\n",
    ").withColumn(\"Exploded_Position\", F.explode(F.col(\"Positions\")))\n",
    "\n",
    "#join exploded_df with sorted_up2_steps_positions_df sorted_up2_steps_positions_df.Position == exploded_df.Exploded_Position\n",
    "joined_df = exploded_df.join(sorted_up2_steps_positions_df, F.col(\"Exploded_Position\") == F.col(\"Position\"), \"inner\")\n",
    "position_title = joined_df.select(\"Cluster_Title\", \"Position\")\n",
    "joined_df_drop_cols = [\"Cluster\", \"Exploded_Position\", \"Position\"]\n",
    "joined_df = joined_df.drop(*joined_df_drop_cols)\n",
    "\n",
    "joined_df_final = joined_df.groupBy(\"Cluster_Title\", \"Positions\").agg(\n",
    "    F.sum(\"Total Count\").alias(\"Number of Users\"),\n",
    "    F.avg(\"avg_steps_before_dream_job\").alias(\"Average Steps Before Dream Job\")\n",
    ")\n",
    "\n",
    "joined_df_final.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea6e9211-4eb6-4e37-84db-9800b312c4cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the table above, you can see the final solution. I suggest to create a basic dashboard\\ active word cloud with this info.\n",
    "\n",
    "In the table below, there are people that can be people we are reccomending the user to connect with, based of his way to his dream job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8519631-4d96-4950-98e0-766b69a28155",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select relevant columns\n",
    "relevant_people_path = relevant_people_path.select(\"name\", \"id\", \"step_title\", \"steps_before_dream_job\")\n",
    "\n",
    "relevant_people_path = relevant_people_path.join(profiels_followers, on = \"id\")\n",
    "\n",
    "relevant_people_path = relevant_people_path.join(position_title, relevant_people_path.step_title == position_title.Position, \"inner\")\n",
    "# Display the first 10 rows\n",
    "relevant_people_path.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37c8dca4-dbc1-4e93-96d4-2e72eb8e572e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Project - Career Path Recommendation & People to Connect With",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}