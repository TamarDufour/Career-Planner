{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f70ac1c6-203f-424d-9387-1ea345f97fe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Internal Mobility\n",
    "\n",
    "In this section, we have created a metric to evaluate internal mobility within companies.\n",
    "Due to the large amount of data, we decided to calculate the score only for companies that currently have a LinkedIn user working in the user's dream position.\n",
    "\n",
    "For example, if the user selected \"Data Scientist\" as their dream job, and the company Dagan & Dror has never hired a Data Scientist, then we would not calculate the metric for this company.\n",
    "\n",
    "The metric for a company is calculated as follows:\n",
    "\n",
    "Average number of users who changed their position within the company in the last two years\n",
    "÷\n",
    "Number of LinkedIn users currently working at this company\n",
    "\n",
    "\n",
    "Assumptions:\n",
    "1. Companies with fewer than 50 employees are not relevant to this metric.\n",
    "2. The number of employees in a company is estimated based on the number of employees listed on LinkedIn. For some companies, this number may be incomplete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd912fa4-8658-41a2-a72b-328944b4f0db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import sparknlp\n",
    "from pyspark.sql.functions import regexp_extract, col, when, explode, collect_list, struct, map_from_entries, size, lower, regexp_replace, to_date, sum, avg, split, lit, expr\n",
    "from pyspark.sql import Window\n",
    "\n",
    "spark.catalog.clearCache()\n",
    "spark = sparknlp.start()\n",
    "profiles = spark.read.parquet('/dbfs/linkedin_people_train_data')\n",
    "companies = spark.read.parquet('/dbfs/linkedin_train_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9881ea9-64f1-4d29-84fa-6f3341ffefd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols_to_drop = [\"about\", \"avatar\", \"city\", \"followers\", \"following\", \"groups\", \"id\", \"languages\", \"people_also_viewed\", \"posts\", \"recommendations\", \"recommendations_count\", \"timestamp\", \"url\", \"certifications\", \"country_code\", \"education\", \"education_details\", \"courses\"]\n",
    "profiles = profiles.drop(*cols_to_drop)\n",
    "profiles_exploded = profiles.withColumn(\"exploded_company_id\", explode(col(\"experience.company_id\"))).filter(col(\"exploded_company_id\").isNotNull())\n",
    "profiles_exploded.limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47577bed-360f-4b4b-a3d1-022e41aa33bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dream_job = 'data scientist'\n",
    "#dream_job = 'data engineer'\n",
    "dream_job = 'hrbp'\n",
    "#dream_job = 'data analyst'\n",
    "\n",
    "companies_dream_job = profiles_exploded.filter(\n",
    "    col(\"position\").like(\"%\" + dream_job + \"%\")\n",
    ").select(\"current_company.name\", \"current_company.company_id\")\n",
    "\n",
    "# Use `companies_dream_job` to filter relevant profiles\n",
    "relevant_profiles = profiles_exploded.join(\n",
    "    companies_dream_job,\n",
    "    profiles_exploded[\"exploded_company_id\"] == companies_dream_job[\"company_id\"],\n",
    "    \"inner\"\n",
    ").select(profiles_exploded[\"*\"])  # Select all columns from `profiles_exploded`\n",
    "relevant_profiles.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3501a52-5377-439c-ae35-dcf533557971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "relevant_profiels = relevant_profiles.select(\"name\", \"experience\")\n",
    "filtered_profiles = relevant_profiels.withColumn(\"exploded_company_id\", explode(col(\"experience.company_id\")))\n",
    "\n",
    "# Step 1: Explode the 'experience' array to get each experience entry as a separate row\n",
    "exploded_df = profiles.withColumn(\"exploded_experience\", explode(col(\"experience\")))\n",
    "\n",
    "# Step 2: Extract relevant fields (name, company, and positions)\n",
    "extracted_df = exploded_df.select(\n",
    "    col(\"name\"),\n",
    "    col(\"exploded_experience.company\").alias(\"company\"),\n",
    "    explode(col(\"exploded_experience.positions\")).alias(\"position_details\")\n",
    ")\n",
    "\n",
    "# Step 3: Extract position title and duration (or tenure) for mapping\n",
    "positions_with_start_date = extracted_df.select(\n",
    "    col(\"name\"),\n",
    "    col(\"company\"),\n",
    "    col(\"position_details.title\").alias(\"position_title\"),\n",
    "    col(\"position_details.start_date\").alias(\"position_start_date\")  # You can use another field if needed\n",
    ")\n",
    "\n",
    "relevant_companies = companies.select(\"name\", \"company_size\", \"employees_in_linkedin\")\n",
    "\n",
    "companies_size = relevant_companies.dropna(subset =[\"company_size\", \"employees_in_linkedin\"])\n",
    "\n",
    "# Define regex patterns to extract X (minimum value) and Y (maximum value)\n",
    "min_pattern = r\"(\\d[\\d,]*)\"         # Matches the first number (X)\n",
    "max_pattern = r\"-(\\d[\\d,]*)\"        # Matches the second number (Y) after '-'\n",
    "plus_pattern = r\"(\\d[\\d,]*)\\+\"      # Matches the number (X) before '+'\n",
    "\n",
    "# Extract min_employees\n",
    "companies_size = companies_size.withColumn(\n",
    "    \"min_employees\",\n",
    "    when(col(\"company_size\").rlike(r\"\\+\"),  # Check if the value has '+'\n",
    "         regexp_extract(col(\"company_size\"), plus_pattern, 1))\n",
    "    .otherwise(regexp_extract(col(\"company_size\"), min_pattern, 1))\n",
    "    .cast(\"int\")  # Convert to integer\n",
    ")\n",
    "\n",
    "# Extract max_employees\n",
    "companies_size = companies_size.withColumn(\n",
    "    \"max_employees\",\n",
    "    when(col(\"company_size\").rlike(r\"\\+\"),  # If there's '+', max_employees is null\n",
    "         None)\n",
    "    .otherwise(regexp_extract(col(\"company_size\"), max_pattern, 1).cast(\"int\"))\n",
    ")\n",
    "\n",
    "# Remove commas from min_employees and max_employees\n",
    "companies_size = companies_size.withColumn(\"min_employees\", regexp_extract(col(\"min_employees\"), r\"(\\d+)\", 0).cast(\"int\"))\n",
    "companies_size = companies_size.withColumn(\"max_employees\", regexp_extract(col(\"max_employees\"), r\"(\\d+)\", 0).cast(\"int\"))\n",
    "\n",
    "#keep only the rows where min_employees<=employees_in_linkedin<=max_employees\n",
    "companies_size = companies_size.filter((companies_size[\"min_employees\"] <= companies_size[\"employees_in_linkedin\"]) & (companies_size[\"employees_in_linkedin\"] <= companies_size[\"max_employees\"]))\n",
    "\n",
    "relevant_companies = companies_size.select(\"name\",\"employees_in_linkedin\")\n",
    "\n",
    "\n",
    "# Step 2: Filter relevant companies\n",
    "relevant_companies = relevant_companies.filter(\n",
    "    (col(\"employees_in_linkedin\").isNotNull()) & (col(\"employees_in_linkedin\") > 50)\n",
    ")\n",
    "\n",
    "relevant_companies = relevant_companies.withColumnRenamed(\"name\", \"company_name\")\n",
    "\n",
    "print(\"There are\", relevant_companies.count(), \"companies with more than 50 employees in LinkedIn.\")\n",
    "\n",
    "positions_with_start_date = positions_with_start_date.join(\n",
    "    relevant_companies,\n",
    "    lower(positions_with_start_date[\"company\"]) == lower(relevant_companies[\"company_name\"]),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "positions_with_start_date = positions_with_start_date.select(\"name\", \"company\", \"position_title\", \"position_start_date\", \"employees_in_linkedin\")\n",
    "positions_with_start_date.limit(10).display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f76c7d2-37bb-4e22-bed5-05aa92071207",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Extract year based on the number of words in position_start_date\n",
    "positions_with_start_year = positions_with_start_date.withColumn(\n",
    "    \"start_year\",\n",
    "    when(size(split(col(\"position_start_date\"), \" \")) == 1, col(\"position_start_date\").cast(\"int\"))  # Single word (Year)\n",
    "    .when(size(split(col(\"position_start_date\"), \" \")) == 2, split(col(\"position_start_date\"), \" \")[1].cast(\"int\"))  # Two words (Month Year)\n",
    ")\n",
    "positions_with_start_month_year = positions_with_start_year.withColumn(\n",
    "    \"start_month\",\n",
    "    when(size(split(col(\"position_start_date\"), \" \")) == 1, \"Jan\")  # Single word (Only Year) → Default to \"Jan\"\n",
    "    .when(size(split(col(\"position_start_date\"), \" \")) == 2, split(col(\"position_start_date\"), \" \")[0])  # Two words → Extract Month\n",
    ")\n",
    "\n",
    "max_year = positions_with_start_month_year.agg({\"start_year\": \"max\"}).collect()[0][\"max(start_year)\"]\n",
    "# Show results\n",
    "positions_with_start_month_year.limit(10).display()\n",
    "print(\"Max year:\", max_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20cf440e-514b-4dbc-924f-d135d271549a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define mapping for month names to numeric values\n",
    "month_mapping = {\n",
    "    \"Jan\": \"01\", \"Feb\": \"02\", \"Mar\": \"03\", \"Apr\": \"04\", \"May\": \"05\", \"Jun\": \"06\",\n",
    "    \"Jul\": \"07\", \"Aug\": \"08\", \"Sep\": \"09\", \"Oct\": \"10\", \"Nov\": \"11\", \"Dec\": \"12\"\n",
    "}\n",
    "\n",
    "# Convert start_month from text to numeric values\n",
    "positions_with_start_month_year = positions_with_start_month_year.withColumn(\n",
    "    \"start_month_numeric\",\n",
    "    when(col(\"start_month\") == \"Jan\", \"01\")\n",
    "    .when(col(\"start_month\") == \"Feb\", \"02\")\n",
    "    .when(col(\"start_month\") == \"Mar\", \"03\")\n",
    "    .when(col(\"start_month\") == \"Apr\", \"04\")\n",
    "    .when(col(\"start_month\") == \"May\", \"05\")\n",
    "    .when(col(\"start_month\") == \"Jun\", \"06\")\n",
    "    .when(col(\"start_month\") == \"Jul\", \"07\")\n",
    "    .when(col(\"start_month\") == \"Aug\", \"08\")\n",
    "    .when(col(\"start_month\") == \"Sep\", \"09\")\n",
    "    .when(col(\"start_month\") == \"Oct\", \"10\")\n",
    "    .when(col(\"start_month\") == \"Nov\", \"11\")\n",
    "    .when(col(\"start_month\") == \"Dec\", \"12\")\n",
    ")\n",
    "\n",
    "# Construct proper date using numeric month values\n",
    "positions_with_start_date_final = positions_with_start_month_year.withColumn(\n",
    "    \"start_date_date\",\n",
    "    to_date(concat_ws(\"-\", \"start_year\", \"start_month_numeric\", lit(\"01\")), \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "# Show results\n",
    "positions_with_start_date_final.limit(10).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16cb5558-2c09-4024-a34a-8e42d9b5a9e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a window specification to sort positions by start date\n",
    "windowSpec = Window.partitionBy(\"name\", \"company\").orderBy(\"start_date_date\")\n",
    "\n",
    "# Group by name and company, collecting sorted positions\n",
    "result_df = (\n",
    "    positions_with_start_date_final.withColumn(\n",
    "        \"sorted_positions\", struct(col(\"start_date_date\"), col(\"position_title\"))\n",
    "    )\n",
    "    .groupBy(\"name\", \"company\")\n",
    "    .agg(\n",
    "        collect_list(\"sorted_positions\").alias(\"positions\"),\n",
    "        collect_list(\"start_date_date\").alias(\"start_dates\")  # Collect all start dates in a list\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Extract only position_title from the struct list\n",
    "result_df = result_df.withColumn(\n",
    "    \"positions\",\n",
    "    col(\"positions\").getField(\"position_title\")\n",
    ")\n",
    "\n",
    "\n",
    "# Keep only results where the length of start_dates list is at least 2\n",
    "result_df = result_df.where(size(col(\"start_dates\")) >= 2)\n",
    "\n",
    "# Ensure at least one of the dates is in max_year or max_year - 1\n",
    "result_df = result_df.where(\n",
    "    expr(f\"exists(start_dates, x -> year(x) = {max_year} OR year(x) = {max_year - 1})\")\n",
    ")\n",
    "\n",
    "# Show the final DataFrame\n",
    "result_df.display()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eff116d8-7b5d-4d92-83ee-5a92d73a2803",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a window specification to sort positions by start date\n",
    "windowSpec = Window.partitionBy(\"name\", \"company\").orderBy(\"position_start_date\")\n",
    "\n",
    "# Group by name and company, collecting sorted positions\n",
    "result_df = (\n",
    "    positions_with_start_date.withColumn(\"sorted_positions\", struct(col(\"position_start_date\"), col(\"position_title\")))\n",
    "    .groupBy(\"name\", \"company\")\n",
    "    .agg(collect_list(\"sorted_positions\").alias(\"positions\"))\n",
    ")\n",
    "\n",
    "# Extract only position_title from the struct list\n",
    "result_df = result_df.withColumn(\n",
    "    \"positions\",\n",
    "    col(\"positions\").getField(\"position_title\")\n",
    ")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "result_df.limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "041e826f-fc01-4499-96c4-d170d2e7f13b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2584"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c1d1631-8561-4e32-9202-f1a1cbb60faa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = result_df.select(\"company\", \"positions\")\n",
    "results_df = result_df.withColumn(\"num_positions_changes\", size(col(\"positions\"))-1)\n",
    "results_df_grouped = results_df.select(\"company\", \"num_positions_changes\")\n",
    "results_df_grouped_sum = results_df_grouped.groupBy(\"company\").agg(sum(\"num_positions_changes\").alias(\"total_num_positions_changes\"))\n",
    "\n",
    "results_df_grouped_avg = results_df_grouped.groupBy(\"company\").agg(avg(\"num_positions_changes\").alias(\"avg_num_positions_changes\"))\n",
    "\n",
    "# Join the two DataFrames on the \"company\" column\n",
    "results_df_grouped = results_df_grouped_sum.join(results_df_grouped_avg, on=\"company\")\n",
    "results_df_grouped = results_df_grouped.withColumnRenamed(\"avg_num_positions_changes\", \"average positions change\")\n",
    "display(results_df_grouped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c85b650-bf20-4678-a1be-cb27a1c1c4c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform the join by comparing company names in lowercase\n",
    "final_IM = results_df_grouped.join(\n",
    "    relevant_companies,\n",
    "    lower(relevant_companies[\"company_name\"]) == lower(results_df_grouped[\"company\"]),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "final_IM = final_IM.select(\"company\", \"total_num_positions_changes\", \"employees_in_linkedin\", \"average positions change\")\n",
    "final_IM = final_IM.withColumn(\"percentage_change\", col(\"total_num_positions_changes\") / col(\"employees_in_linkedin\") * 100)\n",
    "\n",
    "# Show the result\n",
    "final_IM.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc2a46d5-3cfd-440e-8085-228cdb364dd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "companies_amazon = companies.filter((companies[\"name\"]) == \"Amazon\")\n",
    "\n",
    "companies_amazon.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Project - Internal Mobility",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}